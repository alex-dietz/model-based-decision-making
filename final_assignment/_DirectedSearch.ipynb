{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Search\n",
    "Likely to be MORDM\n",
    "1) Initialize Model/ Problem Specification\n",
    "2) Search Candidate Solutions (based on Ref Scenario)\n",
    "3) Re-Evaluate Solutions under Uncertainty (Many Scenario)\n",
    "4) Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/ema_workbench/analysis/prim.py:30: ImportWarning: altair based interactive inspection not available\n",
      "  warnings.warn((\"altair based interactive \" \"inspection not available\"), ImportWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger EMA (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# EMA\n",
    "from ema_workbench import (Model, RealParameter,  Scenario, MultiprocessingEvaluator, ScalarOutcome, perform_experiments, ema_logging)\n",
    "from ema_workbench import Samplers, Policy\n",
    "from ema_workbench import save_results, load_results\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, HyperVolume\n",
    "from ema_workbench.analysis import parcoords\n",
    "# Model\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------- Set Model Parameters\n",
    "problem_formulation = 6\n",
    "model, steps = get_model_for_problem_formulation(problem_formulation)\n",
    "\n",
    "uncertainties = model.uncertainties\n",
    "levers = model.levers\n",
    "outcomes = model.outcomes\n",
    "\n",
    "#------------------------- Reference Scenario\n",
    "ref_val = {'Bmax': 175, \n",
    "           'Brate': 1.5, \n",
    "           'pfail': 0.5,\n",
    "            'discount rate': 3.5,\n",
    "            'ID flood wave shape': 4}\n",
    "\n",
    "ref_dict = {}\n",
    "# < ref_dict >\n",
    "# reference scenario updated for all dike rings\n",
    "for key in model.uncertainties:\n",
    "    name_split = key.name.split('_')\n",
    "    if len(name_split) == 1:\n",
    "        if key.name in ref_val.keys():\n",
    "            ref_dict.update({key.name: ref_val[key.name]})\n",
    "    else:\n",
    "        ref_dict.update({key.name: ref_val[name_split[1]]})\n",
    "\n",
    "\n",
    "#------------------------- Optimization Parameters\n",
    "\n",
    "ref_scenario = Scenario('reference', **ref_dict)\n",
    "\n",
    "convergence_metrics = [EpsilonProgress()]\n",
    "nfe = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntegerParameter('0_RfR 0', 0, 1, resolution=None, default=None, variable_name=['0_RfR 0'], pff=False)\n",
      "IntegerParameter('1_RfR 0', 0, 1, resolution=None, default=None, variable_name=['1_RfR 0'], pff=False)\n",
      "IntegerParameter('2_RfR 0', 0, 1, resolution=None, default=None, variable_name=['2_RfR 0'], pff=False)\n",
      "IntegerParameter('3_RfR 0', 0, 1, resolution=None, default=None, variable_name=['3_RfR 0'], pff=False)\n",
      "IntegerParameter('4_RfR 0', 0, 1, resolution=None, default=None, variable_name=['4_RfR 0'], pff=False)\n",
      "IntegerParameter('EWS_DaysToThreat', 0, 0, resolution=None, default=None, variable_name=['EWS_DaysToThreat'], pff=False)\n",
      "IntegerParameter('A.1_DikeIncrease 0', 0, 10, resolution=None, default=None, variable_name=['A.1_DikeIncrease 0'], pff=False)\n",
      "IntegerParameter('A.2_DikeIncrease 0', 0, 10, resolution=None, default=None, variable_name=['A.2_DikeIncrease 0'], pff=False)\n",
      "IntegerParameter('A.3_DikeIncrease 0', 0, 10, resolution=None, default=None, variable_name=['A.3_DikeIncrease 0'], pff=False)\n",
      "IntegerParameter('A.4_DikeIncrease 0', 0, 10, resolution=None, default=None, variable_name=['A.4_DikeIncrease 0'], pff=False)\n",
      "IntegerParameter('A.5_DikeIncrease 0', 0, 10, resolution=None, default=None, variable_name=['A.5_DikeIncrease 0'], pff=False)\n"
     ]
    }
   ],
   "source": [
    "for policy in model.levers:\n",
    "    print(repr(policy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Search Candidate Solutions (based on Ref Scenario)\n",
    "- Run Optimization on Lever Space\n",
    "- Find Candidate Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "with MultiprocessingEvaluator(model, n_processes = 6) as evaluator:\n",
    "    results1, convergence = evaluator.optimize(nfe=nfe,\n",
    "                                            searchover='levers',\n",
    "                                            epsilons=[0.1]*len(model.outcomes),\n",
    "                                              convergence=convergence_metrics,\n",
    "                                            reference=ref_scenario\n",
    "                                            )\n",
    "    \n",
    "results1.to_csv('results/optimization_{}_{}.csv'.format(problem_formulation,nfe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final result data set\n",
    "results = pd.read_csv(\"results/optimization_6_200.csv\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives results across each of the dike rings for problem_formulation (3)\n",
    "for i in range(5):\n",
    "\n",
    "        data = results.loc[:, [f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               ]]\n",
    "        \n",
    "        limits = parcoords.get_limits(data)\n",
    "        limits.loc[0, [f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               ]] = 0\n",
    "\n",
    "        paraxes = parcoords.ParallelAxes(limits)\n",
    "        paraxes.plot(data)\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Re-Evaluate Solutions under Uncertainty (Few Candidate Policies, Many Scenario)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a Constrain Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create aggregate columns for death, damage and investment cost\n",
    "results[\"Gelderland_Deaths\"] = results[[f\"A.{i+1}_Expected Number of Deaths\" for i in range(3)]].sum(axis=1)\n",
    "results[\"Gelderland_Damages\"] = results[[f\"A.{i+1}_Expected Annual Damage\" for i in range(3)]].sum(axis=1)\n",
    "results[\"Gelderland_Investment\"] = results[[f\"A.{i+1}_Dike Investment Costs\" for i in range(3)]].sum(axis=1)\n",
    "results[\"Overijssel_Deaths\"] = results[[f\"A.{i}_Expected Number of Deaths\" for i in [4, 5]]].sum(axis=1)\n",
    "results[\"Overijssel_Damages\"] = results[[f\"A.{i}_Expected Annual Damage\" for i in [4, 5]]].sum(axis=1)\n",
    "results[\"Overijssel_Investment\"] = results[[f\"A.{i}_Dike Investment Costs\" for i in [4, 5]]].sum(axis=1)\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set threshiold for priority 1, 2 and 3: Deaths, Damages and Investment costs\n",
    "distribution_threshold_deaths = 0.1\n",
    "distribution_threshold_damages = 0.1\n",
    "distribution_threshold_investment = 0.1\n",
    "#Refine Policies-Dataframe to fit Priority 1 criterion\n",
    "results2 = results.sort_values(\"Gelderland_Deaths\")\n",
    "results2 = results.iloc[0:int(len(results2)*distribution_threshold_deaths),:]\n",
    "\n",
    "#Refine Policies-Dataframe to fit Priority 2 criterion\n",
    "results3 = results2.sort_values(\"Gelderland_Damages\")\n",
    "results3 = results3.iloc[0:int(len(results3)*distribution_threshold_damages),:]\n",
    "\n",
    "#Refine Policies-Dataframe to fit Priority 3 criterion\n",
    "results4 = results3.sort_values(\"Gelderland_Investment\")\n",
    "policies = results4.iloc[0:int(len(results4)*distribution_threshold_investment),:]\n",
    "\n",
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives results across each of the dike rings for problem_formulation (3)\n",
    "for i in range(5):\n",
    "\n",
    "        data = policies.loc[:, [f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               ]]\n",
    "        \n",
    "        limits = parcoords.get_limits(data)\n",
    "        limits.loc[0, [f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               ]] = 0\n",
    "\n",
    "        paraxes = parcoords.ParallelAxes(limits)\n",
    "        paraxes.plot(data)\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b Robustness Analysis (Run Scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_scenarios = 1000\n",
    "# Policies are subsequently collected in a list as policy objects\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    rob_results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n",
    "    \n",
    "save_results(rob_results, 'results/MORDM2.tar.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_results = load_results ('results/MORDM2.tar.gz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c Visualize Results\n",
    "1) Signal to Noise Ratio\n",
    "2) Calculate Maximum regret\n",
    "3) Scenario Dicovery to understand bad conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3c1 Signal to Noise Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal to noise - high value for maximizable outcomes is desirable\n",
    "#outcome to have high average value and low standard deviation\n",
    "\n",
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = rob_results\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    \n",
    "    logical = experiments['policy']==policy\n",
    "    \n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()\n",
    "#gives results across each of the dike rings for problem_formulation (3)\n",
    "\"\"\" for i in range(5):\n",
    "\n",
    "        data = policies.loc[:, [f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               ]]\n",
    "        \n",
    "        limits = parcoords.get_limits(data)\n",
    "        limits.loc[0, [f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               ]] = 0\n",
    "\n",
    "        paraxes = parcoords.ParallelAxes(limits)\n",
    "        paraxes.plot(data)\n",
    "        \n",
    "        \n",
    "        plt.show() \"\"\"\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3c2 Calculate Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret(data, best):\n",
    "    return np.abs(best-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = rob_results\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in model.outcomes:\n",
    "    policy_column = experiments['policy']\n",
    "    \n",
    "    # create a DataFrame with all the relevant information\n",
    "    # i.e., policy, scenario_id, and scores\n",
    "    data = pd.DataFrame({outcome.name: outcomes[outcome.name], \n",
    "                         \"policy\":experiments['policy'],\n",
    "                         \"scenario\":experiments['scenario']})\n",
    "    \n",
    "    # reorient the data by indexing with policy and scenario id\n",
    "    data = data.pivot(index='scenario', columns='policy')\n",
    "    \n",
    "    # flatten the resulting hierarchical index resulting from \n",
    "    # pivoting, (might be a nicer solution possible)\n",
    "    data.columns = data.columns.get_level_values(1)\n",
    "    \n",
    "    # Convert your DataFrame to a numpy array before multi-dimensional indexing\n",
    "    data_array = data.to_numpy()\n",
    "\n",
    "    # we need to control the broadcasting. \n",
    "    # max returns a 1d vector across scenario id. By passing\n",
    "    # np.newaxis we ensure that the shape is the same as the data\n",
    "    # next we take the absolute value\n",
    "    #\n",
    "    # basically we take the difference of the maximum across \n",
    "    # the row and the actual values in the row\n",
    "    #\n",
    "    outcome_regret = np.abs(data_array.max(axis=1)[:, np.newaxis] - data_array)\n",
    "    # Convert numpy array back to DataFrame\n",
    "    outcome_regret_df = pd.DataFrame(outcome_regret, index=data.index, columns=data.columns)\n",
    "    \n",
    "    overall_regret[outcome.name] = outcome_regret_df\n",
    "    max_regret[outcome.name] = outcome_regret_df.max()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize as heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_regret = pd.DataFrame(max_regret)\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize as parallel axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "\n",
    "data = max_regret\n",
    "\n",
    "# makes it easier to identify the policy associated with each line\n",
    "# in the parcoords plot\n",
    "# data['policy'] = data.index.astype(\"float64\")\n",
    "\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "for i, (index, row) in enumerate(data.iterrows()):\n",
    "    paraxes.plot(row.to_frame().T, label=str(index), color=colors[i])\n",
    "paraxes.legend()\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
